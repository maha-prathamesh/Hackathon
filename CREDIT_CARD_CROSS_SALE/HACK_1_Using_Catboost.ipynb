{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and perform basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(DATA_DIR):\n",
    "    \n",
    "    train = pd.read_csv(DATA_DIR+\"train_Df64byy.csv\")\n",
    "    test = pd.read_csv(DATA_DIR+\"test_YCcRUnU.csv\")\n",
    "    \n",
    "    #Removes train rows which has Region_Code not present in test set\n",
    "    test_region_list=test['Region_Code'].tolist()\n",
    "    train=train[train['Region_Code'].isin(test_region_list)]\n",
    "    \n",
    "    train['train_or_test']='train'\n",
    "    test['train_or_test']='test'\n",
    "    df=pd.concat([train,test])\n",
    "    \n",
    "    df['Holding_Policy_Duration']=(df['Holding_Policy_Duration'].replace(['14+'],[15])).astype(float)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    for col in ['City_Code','Accomodation_Type','Reco_Insurance_Type','Health Indicator','Is_Spouse']:\n",
    "        df[col]=  df[col].astype('str')\n",
    "        df[col]= le.fit_transform(df[col])\n",
    "        \n",
    "\n",
    "    \n",
    "    return train,test,df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_encoding(column_name,output_column_name,df):\n",
    "    fe_pol = (df.groupby(column_name).size()) / len(df)\n",
    "    df[output_column_name] = df[column_name].apply(lambda x : fe_pol[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_encoding(column_name,output_column_name,df):\n",
    "    fe_pol = (df.groupby(column_name).size()) / len(df)\n",
    "    df[output_column_name] = df[column_name].apply(lambda x : fe_pol[x])\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "     #Interaction Feature (Combining 2 categorical features and performing frequency encoding)\n",
    "        \n",
    "    cat_features=[]\n",
    "    le_features=[]\n",
    "    columns=['City_Code','Accomodation_Type','Reco_Insurance_Type','Health Indicator','Is_Spouse','Region_Code','Holding_Policy_Type','Reco_Policy_Cat']\n",
    "\n",
    "    comb = combinations(columns, 2) \n",
    "\n",
    "    for i in list(comb):  \n",
    "        df[f'{i[0]}_{i[1]}']=df[i[0]].astype(str)+'_'+df[i[1]].astype(str)\n",
    "        df[f'{i[0]}_{i[1]}_le']=le.fit_transform(df[f'{i[0]}_{i[1]}'])\n",
    "        le_features.append(f'{i[0]}_{i[1]}_le')\n",
    "        frequency_encoding(f'{i[0]}_{i[1]}',f'{i[0]}_{i[1]}',df)\n",
    "        cat_features.append(f'{i[0]}_{i[1]}')   \n",
    "        \n",
    "    #Frequency Encoding\n",
    "    \n",
    "    frequency_encoding('Region_Code','Region_Code_fe',df)\n",
    "    frequency_encoding('Reco_Policy_Cat','Reco_Policy_Cat_fe',df)\n",
    "    \n",
    "    #Deriving characteristics of each city by creating aggregate features\n",
    "    \n",
    "    city_aggregate_features = df.groupby(['City_Code']).agg({'Lower_Age': ['mean', 'max', 'min','std'],\n",
    "                                                     'Reco_Policy_Premium': ['mean', 'max', 'min','std'], \n",
    "                                                     'Region_Code': ['nunique','count'], \n",
    "                                                     'Accomodation_Type': ['nunique','count'],\n",
    "                                                     'Reco_Insurance_Type': ['nunique','count'] ,\n",
    "                                                     'Health Indicator': ['nunique','count'] ,\n",
    "                                                     'Holding_Policy_Type': ['nunique','count'] ,\n",
    "                                                     'Reco_Policy_Cat': ['nunique','count'] ,\n",
    "                                                     })\n",
    "    city_aggregate_features.columns = ['city_aggregate_features' + '_'.join(c).strip('_') for c in city_aggregate_features.columns]\n",
    "    df = pd.merge(df, city_aggregate_features, on = ['City_Code'], how='left')\n",
    "\n",
    " \n",
    "    city_region_aggregate_features = df.groupby(['City_Code','Region_Code']).agg({'Lower_Age': ['mean', 'max', 'min','std'],\n",
    "                                                     'Reco_Policy_Premium': ['mean', 'max', 'min','std'],  \n",
    "                                                     'Accomodation_Type': ['nunique','count'],\n",
    "                                                     'Reco_Insurance_Type': ['nunique','count'] ,\n",
    "                                                     'Health Indicator': ['nunique','count'] ,\n",
    "                                                     'Holding_Policy_Type': ['nunique','count'] ,\n",
    "                                                     'Reco_Policy_Cat': ['nunique','count'] ,\n",
    "                                                     })\n",
    "    city_region_aggregate_features.columns = ['city_region_aggregate_features' + '_'.join(c).strip('_') for c in city_region_aggregate_features.columns]\n",
    "    df = pd.merge(df, city_region_aggregate_features, on = ['City_Code','Region_Code'], how='left')\n",
    "\n",
    "    city_recopolicycat_aggregate_features = df.groupby(['City_Code','Reco_Policy_Cat']).agg({'Lower_Age': ['mean', 'max', 'min','std'],\n",
    "                                                     'Reco_Policy_Premium': ['mean', 'max', 'min','std'], \n",
    "                                                     'Region_Code': ['nunique','count'], \n",
    "                                                     'Accomodation_Type': ['nunique','count'],\n",
    "                                                     'Reco_Insurance_Type': ['nunique','count'] ,\n",
    "                                                     'Health Indicator': ['nunique','count'] ,\n",
    "                                                     'Holding_Policy_Type': ['nunique','count'] \n",
    "                                                     })\n",
    "    city_recopolicycat_aggregate_features.columns = ['city_recopolicycat_aggregate_features' + '_'.join(c).strip('_') for c in city_recopolicycat_aggregate_features.columns]\n",
    "    df = pd.merge(df, city_recopolicycat_aggregate_features, on = ['City_Code','Reco_Policy_Cat'], how='left')\n",
    "    \n",
    "    city_regioncoderecopolicycat_aggregate_features = df.groupby(['City_Code','Region_Code_Reco_Policy_Cat']).agg({'Lower_Age': ['mean', 'max', 'min','std'],\n",
    "                                                     'Reco_Policy_Premium': ['mean', 'max', 'min','std'], \n",
    "                                                     'Region_Code': ['nunique','count'], \n",
    "                                                     'Accomodation_Type': ['nunique','count'],\n",
    "                                                     'Reco_Insurance_Type': ['nunique','count'] ,\n",
    "                                                     'Health Indicator': ['nunique','count'] ,\n",
    "                                                     'Holding_Policy_Type': ['nunique','count'] ,\n",
    "                                                     'Reco_Policy_Cat': ['nunique','count'] ,\n",
    "                                                     })\n",
    "\n",
    "    city_regioncoderecopolicycat_aggregate_features.columns = ['city_regioncoderecopolicycat_aggregate_features' + '_'.join(c).strip('_') for c in city_regioncoderecopolicycat_aggregate_features.columns]\n",
    "    df = pd.merge(df, city_regioncoderecopolicycat_aggregate_features, on = ['City_Code','Region_Code_Reco_Policy_Cat'], how='left')\n",
    "    \n",
    "    for i in cat_features:\n",
    "        df[f'city_{i}_max']=df.groupby('City_Code')[i].transform('max')\n",
    "        df[f'city_{i}_min']=df.groupby('City_Code')[i].transform('min')\n",
    "        df[f'city_{i}_mean']=df.groupby('City_Code')[i].transform('mean')\n",
    "        df[f'city_{i}_std']=df.groupby('City_Code')[i].transform('std')\n",
    "\n",
    "    \n",
    "        df[f'city_region_{i}_max']=df.groupby(['City_Code','Region_Code'])[i].transform('max')\n",
    "        df[f'city_region_{i}_min']=df.groupby(['City_Code','Region_Code'])[i].transform('min')\n",
    "        df[f'city_region_{i}_mean']=df.groupby(['City_Code','Region_Code'])[i].transform('mean')\n",
    "        df[f'city_region_{i}_std']=df.groupby(['City_Code','Region_Code'])[i].transform('std')\n",
    "\n",
    "    \n",
    "        df[f'city_recopolicycat_{i}_max']=df.groupby(['City_Code','Reco_Policy_Cat'])[i].transform('max')\n",
    "        df[f'city_recopolicycat_{i}_min']=df.groupby(['City_Code','Reco_Policy_Cat'])[i].transform('min')\n",
    "        df[f'city_recopolicycat_{i}_mean']=df.groupby(['City_Code','Reco_Policy_Cat'])[i].transform('mean')\n",
    "        df[f'city_recopolicycat_{i}_std']=df.groupby(['City_Code','Reco_Policy_Cat'])[i].transform('std')\n",
    "        \n",
    "    \n",
    "    #features on reco_policy_cat\n",
    "    \n",
    "    recopolicycat_aggregate_features = df.groupby(['Reco_Policy_Cat']).agg({'Lower_Age': ['mean', 'max', 'min','std'],\n",
    "                                                     'Reco_Policy_Premium': ['mean', 'max', 'min','std','sum'],   \n",
    "                                                     'Region_Code': ['nunique','count'], \n",
    "                                                     'Accomodation_Type': ['nunique'],\n",
    "                                                     'Reco_Insurance_Type': ['nunique'] ,\n",
    "                                                     'Health Indicator': ['nunique','count'] ,\n",
    "                                                     'Holding_Policy_Type': ['nunique','count'] ,\n",
    "                                                     'City_Code': ['nunique','count'] ,\n",
    "                                                     })\n",
    "    recopolicycat_aggregate_features.columns = ['recopolicycat_aggregate_features' + '_'.join(c).strip('_') for c in recopolicycat_aggregate_features.columns]\n",
    "    df = pd.merge(df, recopolicycat_aggregate_features, on = ['Reco_Policy_Cat'], how='left')\n",
    "        \n",
    "        \n",
    "\n",
    "    #features on Holding_Policy_Type \n",
    "    \n",
    "    holdingpolicytype_aggregate_features = df.groupby(['Holding_Policy_Type']).agg({'Lower_Age': ['mean', 'max', 'min','std'],\n",
    "                                                     'Reco_Policy_Premium': ['mean', 'max', 'min','std'], \n",
    "                                                     'Region_Code': ['nunique','count'], \n",
    "                                                     'Accomodation_Type': ['nunique','count'],\n",
    "                                                     'Reco_Insurance_Type': ['nunique','count'] ,\n",
    "                                                     'Health Indicator': ['nunique','count'] ,\n",
    "                                                     'City_Code': ['nunique','count'] ,\n",
    "                                                     })\n",
    "    holdingpolicytype_aggregate_features.columns = ['holdingpolicytype_aggregate_features' + '_'.join(c).strip('_') for c in holdingpolicytype_aggregate_features.columns]\n",
    "    df = pd.merge(df, holdingpolicytype_aggregate_features, on = ['Holding_Policy_Type'], how='left')\n",
    "    \n",
    "    #Deriving characteristics of Accomodation_Type by creating aggregate features\n",
    "    \n",
    "    Accomodation_Type_aggregate_features = df.groupby(['Accomodation_Type']).agg({'Lower_Age': ['mean', 'max', 'min','std'],\n",
    "                                                     'Reco_Policy_Premium': ['mean', 'max', 'min','std','sum'],   \n",
    "                                                     'Region_Code': ['nunique','count'], \n",
    "                                                     'Reco_Insurance_Type': ['nunique'] ,\n",
    "                                                     'Health Indicator': ['nunique','count'] ,\n",
    "                                                     'Holding_Policy_Type': ['nunique','count'] ,\n",
    "                                                     'City_Code': ['nunique','count'] ,\n",
    "                                                     })\n",
    "    Accomodation_Type_aggregate_features.columns = ['Accomodation_Type_aggregate_features' + '_'.join(c).strip('_') for c in Accomodation_Type_aggregate_features.columns]\n",
    "    df = pd.merge(df, Accomodation_Type_aggregate_features, on = ['Accomodation_Type'], how='left')\n",
    "    \n",
    "    #Deriving characteristics of Interaction_features by creating aggregate features (These interaction feature are selected for aggregating based on its feature importance)\n",
    "    \n",
    "    Region_CodeReco_Policy_Cat_grpd = df.groupby(['Region_Code_Reco_Policy_Cat']).agg({ 'Reco_Policy_Premium': ['mean', 'max', 'min', 'std']})                                                              \n",
    "                                                     \n",
    "    Region_CodeReco_Policy_Cat_grpd.columns = ['grpd_by_Region_Code_Reco_Policy_Cat_' + '_'.join(c).strip('_') for c in Region_CodeReco_Policy_Cat_grpd.columns]\n",
    "    df = pd.merge(df, Region_CodeReco_Policy_Cat_grpd, on = ['Region_Code_Reco_Policy_Cat'], how='left')\n",
    "\n",
    "\n",
    "    City_CodeRegion_Code_grpd = df.groupby(['City_Code_Region_Code']).agg({ 'Reco_Policy_Premium': ['mean', 'max', 'min', 'std']})                                                              \n",
    "                                                     \n",
    "    City_CodeRegion_Code_grpd.columns = ['grpd_by_City_CodeRegion_Code_' + '_'.join(c).strip('_') for c in City_CodeRegion_Code_grpd.columns]\n",
    "    df = pd.merge(df, City_CodeRegion_Code_grpd, on = ['City_Code_Region_Code'], how='left')\n",
    "\n",
    "\n",
    "    City_CodeReco_Policy_Cat_grpd = df.groupby(['City_Code_Reco_Policy_Cat']).agg({ 'Reco_Policy_Premium': ['mean', 'max', 'min', 'std']})                                                              \n",
    "                                                     \n",
    "    City_CodeReco_Policy_Cat_grpd.columns = ['grpd_by_City_CodeReco_Policy_Cat_' + '_'.join(c).strip('_') for c in City_CodeReco_Policy_Cat_grpd.columns]\n",
    "    df = pd.merge(df, City_CodeReco_Policy_Cat_grpd, on = ['City_Code_Reco_Policy_Cat'], how='left')\n",
    "\n",
    "\n",
    "    Holding_Policy_TypeReco_Policy_Cat_grpd = df.groupby(['Holding_Policy_Type_Reco_Policy_Cat']).agg({ 'Reco_Policy_Premium': ['mean', 'max', 'min', 'std']})                                                              \n",
    "                                                     \n",
    "    Holding_Policy_TypeReco_Policy_Cat_grpd.columns = ['grpd_by_Holding_Policy_TypeReco_Policy_Cat_' + '_'.join(c).strip('_') for c in Holding_Policy_TypeReco_Policy_Cat_grpd.columns]\n",
    "    df = pd.merge(df, Holding_Policy_TypeReco_Policy_Cat_grpd, on = ['Holding_Policy_Type_Reco_Policy_Cat'], how='left')\n",
    "    \n",
    "    return df,le_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary columns and prepare the train and test data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparedatafortraining(df,train,test):\n",
    "    \n",
    "    train=df.loc[df.train_or_test.isin(['train'])]\n",
    "    test=df.loc[df.train_or_test.isin(['test'])]\n",
    "    \n",
    "    drop_columns={'ID','Response','Upper_Age','train_or_test'}\n",
    "    \n",
    "    target=['Response']\n",
    "    \n",
    "    x=train.drop(columns=drop_columns,axis=1)\n",
    "    y=train[target]\n",
    "    x_test=test.drop(columns=drop_columns,axis=1)\n",
    "    \n",
    "    print(x.shape)\n",
    "    \n",
    "    return x,y,x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savedata(**DATA_DIR):\n",
    "    \n",
    "    train,test,df=process_data(\"../input/analytics-vidhya-jobathon/\")\n",
    "    df,cat_features=feature_engineering(df)\n",
    "    x_train,y_train,x_test=preparedatafortraining(df,train,test)\n",
    "    \n",
    "    #x_train.to_pickle(\"x_train_lgbm.pkl\")\n",
    "    #y_train.to_pickle(\"y_train_lgbm.pkl\")\n",
    "    #x_test.to_pickle(\"x_test_lgbm.pkl\")\n",
    "    \n",
    "    return x_train,y_train,x_test,cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CatBoost Model and save the validation and test set prediction for ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_model():\n",
    "    \n",
    "    x,y,x_test,cat_features=savedata()\n",
    "     \n",
    "    err = [] \n",
    "\n",
    "    oofs = np.zeros(shape=(len(x)))\n",
    "    preds = np.zeros(shape=(len(x_test)))\n",
    "\n",
    "    Folds=8\n",
    "\n",
    "    fold = StratifiedKFold(n_splits=Folds, shuffle=True, random_state=2020)\n",
    "    i = 1\n",
    "\n",
    "    for train_index, test_index in fold.split(x, y):\n",
    "        x_train, x_val = x.iloc[train_index], x.iloc[test_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "        m =  CatBoostClassifier(n_estimators=10000,random_state=2020,eval_metric='AUC')\n",
    "    \n",
    "        m.fit(x_train, y_train,eval_set=[(x_val, y_val)], early_stopping_rounds=30,verbose=100,cat_features=cat_features)\n",
    "    \n",
    "        pred_y = m.predict_proba(x_val)[:,1]\n",
    "        oofs[test_index] = pred_y\n",
    "        print(i, \" err_cat: \", roc_auc_score(y_val,pred_y))\n",
    "        err.append(roc_auc_score(y_val,pred_y))\n",
    "        preds+= m.predict_proba(x_test)[:,1]\n",
    "        i = i + 1\n",
    "    preds=preds/Folds\n",
    "    \n",
    "    print(f\"Average StratifiedKFold Score : {sum(err)/Folds} \")\n",
    "    oof_score = roc_auc_score(y, oofs)\n",
    "    print(f'\\nOOF Auc is : {oof_score}')\n",
    "    \n",
    "    oofs=pd.DataFrame(oofs,columns=['catboostoof'])\n",
    "    preds=pd.DataFrame(preds,columns=['catboostpred'])\n",
    "    \n",
    "    oofs.to_csv('catboostoof.csv',index=False)\n",
    "    preds.to_csv('catboostpred.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48752, 552)\n",
      "Learning rate set to 0.029412\n",
      "0:\ttest: 0.7636705\tbest: 0.7636705 (0)\ttotal: 336ms\tremaining: 55m 56s\n",
      "100:\ttest: 0.8133179\tbest: 0.8133179 (100)\ttotal: 25.7s\tremaining: 42m 1s\n",
      "200:\ttest: 0.8176711\tbest: 0.8176711 (200)\ttotal: 50.5s\tremaining: 41m 4s\n",
      "300:\ttest: 0.8191514\tbest: 0.8191629 (299)\ttotal: 1m 14s\tremaining: 40m 14s\n",
      "400:\ttest: 0.8204221\tbest: 0.8204221 (400)\ttotal: 1m 39s\tremaining: 39m 34s\n",
      "500:\ttest: 0.8214007\tbest: 0.8214007 (500)\ttotal: 2m 2s\tremaining: 38m 49s\n",
      "600:\ttest: 0.8223918\tbest: 0.8224123 (597)\ttotal: 2m 27s\tremaining: 38m 21s\n",
      "700:\ttest: 0.8231721\tbest: 0.8231721 (700)\ttotal: 2m 51s\tremaining: 37m 49s\n",
      "Stopped by overfitting detector  (30 iterations wait)\n",
      "\n",
      "bestTest = 0.823356621\n",
      "bestIteration = 719\n",
      "\n",
      "Shrink model to first 720 iterations.\n",
      "1  err_cat:  0.8233566210205541\n",
      "Learning rate set to 0.029412\n",
      "0:\ttest: 0.7472616\tbest: 0.7472616 (0)\ttotal: 274ms\tremaining: 45m 36s\n",
      "100:\ttest: 0.7987846\tbest: 0.7987846 (100)\ttotal: 25.6s\tremaining: 41m 52s\n",
      "200:\ttest: 0.8041365\tbest: 0.8041532 (199)\ttotal: 50.6s\tremaining: 41m 4s\n",
      "300:\ttest: 0.8061496\tbest: 0.8061496 (300)\ttotal: 1m 15s\tremaining: 40m 39s\n",
      "400:\ttest: 0.8075186\tbest: 0.8075619 (396)\ttotal: 1m 39s\tremaining: 39m 45s\n",
      "500:\ttest: 0.8083763\tbest: 0.8083763 (500)\ttotal: 2m 3s\tremaining: 39m 6s\n",
      "600:\ttest: 0.8090213\tbest: 0.8090506 (592)\ttotal: 2m 27s\tremaining: 38m 25s\n",
      "700:\ttest: 0.8094298\tbest: 0.8094531 (696)\ttotal: 2m 51s\tremaining: 37m 54s\n",
      "Stopped by overfitting detector  (30 iterations wait)\n",
      "\n",
      "bestTest = 0.8095717816\n",
      "bestIteration = 730\n",
      "\n",
      "Shrink model to first 731 iterations.\n",
      "2  err_cat:  0.8095717816402965\n",
      "Learning rate set to 0.029412\n",
      "0:\ttest: 0.7674204\tbest: 0.7674204 (0)\ttotal: 290ms\tremaining: 48m 20s\n",
      "100:\ttest: 0.8193290\tbest: 0.8193290 (100)\ttotal: 26.1s\tremaining: 42m 41s\n",
      "200:\ttest: 0.8238929\tbest: 0.8238929 (200)\ttotal: 51.1s\tremaining: 41m 31s\n",
      "300:\ttest: 0.8255686\tbest: 0.8255995 (295)\ttotal: 1m 15s\tremaining: 40m 43s\n",
      "400:\ttest: 0.8267162\tbest: 0.8267162 (400)\ttotal: 1m 39s\tremaining: 39m 50s\n",
      "500:\ttest: 0.8273919\tbest: 0.8273978 (499)\ttotal: 2m 3s\tremaining: 39m 8s\n",
      "Stopped by overfitting detector  (30 iterations wait)\n",
      "\n",
      "bestTest = 0.8275902289\n",
      "bestIteration = 547\n",
      "\n",
      "Shrink model to first 548 iterations.\n",
      "3  err_cat:  0.8275902288924218\n",
      "Learning rate set to 0.029412\n",
      "0:\ttest: 0.7490986\tbest: 0.7490986 (0)\ttotal: 271ms\tremaining: 45m 14s\n",
      "100:\ttest: 0.8061060\tbest: 0.8061060 (100)\ttotal: 25.8s\tremaining: 42m 12s\n",
      "200:\ttest: 0.8109615\tbest: 0.8109615 (200)\ttotal: 50.3s\tremaining: 40m 51s\n",
      "300:\ttest: 0.8131443\tbest: 0.8131508 (299)\ttotal: 1m 14s\tremaining: 40m 1s\n",
      "400:\ttest: 0.8143200\tbest: 0.8143200 (400)\ttotal: 1m 38s\tremaining: 39m 21s\n",
      "500:\ttest: 0.8151482\tbest: 0.8152005 (498)\ttotal: 2m 2s\tremaining: 38m 49s\n",
      "600:\ttest: 0.8155098\tbest: 0.8155722 (590)\ttotal: 2m 26s\tremaining: 38m 12s\n",
      "Stopped by overfitting detector  (30 iterations wait)\n",
      "\n",
      "bestTest = 0.8155937385\n",
      "bestIteration = 613\n",
      "\n",
      "Shrink model to first 614 iterations.\n",
      "4  err_cat:  0.8155937385181125\n",
      "Learning rate set to 0.029412\n",
      "0:\ttest: 0.7421195\tbest: 0.7421195 (0)\ttotal: 266ms\tremaining: 44m 16s\n",
      "100:\ttest: 0.7928726\tbest: 0.7928726 (100)\ttotal: 25.9s\tremaining: 42m 22s\n",
      "200:\ttest: 0.7969229\tbest: 0.7969753 (199)\ttotal: 50.5s\tremaining: 41m 3s\n",
      "300:\ttest: 0.7988814\tbest: 0.7988814 (300)\ttotal: 1m 14s\tremaining: 40m 6s\n",
      "400:\ttest: 0.7999655\tbest: 0.7999808 (398)\ttotal: 1m 39s\tremaining: 39m 31s\n",
      "500:\ttest: 0.8009388\tbest: 0.8010527 (492)\ttotal: 2m 3s\tremaining: 38m 57s\n",
      "Stopped by overfitting detector  (30 iterations wait)\n",
      "\n",
      "bestTest = 0.8011298461\n",
      "bestIteration = 522\n",
      "\n",
      "Shrink model to first 523 iterations.\n",
      "5  err_cat:  0.8011298461425779\n",
      "Learning rate set to 0.029412\n",
      "0:\ttest: 0.7544542\tbest: 0.7544542 (0)\ttotal: 282ms\tremaining: 46m 55s\n",
      "100:\ttest: 0.8035522\tbest: 0.8035522 (100)\ttotal: 26s\tremaining: 42m 24s\n",
      "200:\ttest: 0.8074119\tbest: 0.8074248 (197)\ttotal: 50.7s\tremaining: 41m 12s\n",
      "300:\ttest: 0.8094657\tbest: 0.8094657 (300)\ttotal: 1m 15s\tremaining: 40m 18s\n",
      "400:\ttest: 0.8104356\tbest: 0.8104452 (398)\ttotal: 1m 39s\tremaining: 39m 34s\n",
      "500:\ttest: 0.8112730\tbest: 0.8113868 (481)\ttotal: 2m 3s\tremaining: 39m 2s\n",
      "Stopped by overfitting detector  (30 iterations wait)\n",
      "\n",
      "bestTest = 0.8113868107\n",
      "bestIteration = 481\n",
      "\n",
      "Shrink model to first 482 iterations.\n",
      "6  err_cat:  0.8113868107189858\n",
      "Learning rate set to 0.029412\n",
      "0:\ttest: 0.7506176\tbest: 0.7506176 (0)\ttotal: 261ms\tremaining: 43m 31s\n",
      "100:\ttest: 0.8087582\tbest: 0.8087582 (100)\ttotal: 25.8s\tremaining: 42m 6s\n",
      "200:\ttest: 0.8139664\tbest: 0.8139664 (200)\ttotal: 50.8s\tremaining: 41m 16s\n",
      "300:\ttest: 0.8155476\tbest: 0.8155476 (300)\ttotal: 1m 14s\tremaining: 40m 11s\n",
      "400:\ttest: 0.8167622\tbest: 0.8167671 (397)\ttotal: 1m 38s\tremaining: 39m 28s\n",
      "500:\ttest: 0.8175787\tbest: 0.8176114 (499)\ttotal: 2m 3s\tremaining: 38m 56s\n",
      "600:\ttest: 0.8182094\tbest: 0.8182345 (596)\ttotal: 2m 27s\tremaining: 38m 20s\n",
      "700:\ttest: 0.8187951\tbest: 0.8187999 (699)\ttotal: 2m 50s\tremaining: 37m 43s\n",
      "Stopped by overfitting detector  (30 iterations wait)\n",
      "\n",
      "bestTest = 0.8188108537\n",
      "bestIteration = 705\n",
      "\n",
      "Shrink model to first 706 iterations.\n",
      "7  err_cat:  0.8188108536582483\n",
      "Learning rate set to 0.029412\n",
      "0:\ttest: 0.7612739\tbest: 0.7612739 (0)\ttotal: 277ms\tremaining: 46m 10s\n",
      "100:\ttest: 0.8046505\tbest: 0.8046505 (100)\ttotal: 25.8s\tremaining: 42m 11s\n",
      "200:\ttest: 0.8092233\tbest: 0.8092233 (200)\ttotal: 50.9s\tremaining: 41m 20s\n",
      "300:\ttest: 0.8112138\tbest: 0.8112138 (300)\ttotal: 1m 15s\tremaining: 40m 23s\n",
      "400:\ttest: 0.8125318\tbest: 0.8125318 (400)\ttotal: 1m 39s\tremaining: 39m 36s\n",
      "500:\ttest: 0.8134118\tbest: 0.8134118 (500)\ttotal: 2m 2s\tremaining: 38m 49s\n",
      "Stopped by overfitting detector  (30 iterations wait)\n",
      "\n",
      "bestTest = 0.8135413492\n",
      "bestIteration = 529\n",
      "\n",
      "Shrink model to first 530 iterations.\n",
      "8  err_cat:  0.8135413491821599\n",
      "Average StratifiedKFold Score : 0.8151226537216696 \n",
      "\n",
      "OOF Auc is : 0.8150003835630804\n"
     ]
    }
   ],
   "source": [
    "catboost_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
